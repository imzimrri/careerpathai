# Story 1.7: Implement Comet Observability

- **Status:** Done

## Story

- **As a** developer,
- **I want** all RAG queries, LLM calls, and tool actions to be logged and tracked,
- **so that** I can debug and measure the agent's performance.

## Acceptance Criteria

1. The backend logs the start of each career path generation request to Comet.
2. Weaviate RAG retrieval results (context documents) are logged to Comet.
3. FriendliAI LLM prompts and responses are logged to Comet.
4. aci.dev tool calls and results are logged to Comet.
5. Code generation and Daytona validation results are logged to Comet.
6. All logs are grouped into a single trace for each user request.
7. The Comet dashboard displays the complete end-to-end trace with timing information.

## Tasks / Subtasks

- [ ] **Task 1 (AC: 1):** Initialize Comet (Opik) SDK integration.
  - [ ] Install Opik SDK: `pip install opik`
  - [ ] Create function `init_comet_client()` in `api/comet_client.py`
  - [ ] Configure Comet API authentication
  - [ ] Initialize trace for each request
- [ ] **Task 2 (AC: 2):** Log Weaviate RAG retrieval.
  - [ ] Log query parameters (target role, limit, certainty)
  - [ ] Log retrieved documents (titles, categories, count)
  - [ ] Log retrieval latency
- [ ] **Task 3 (AC: 3):** Log FriendliAI LLM interactions.
  - [ ] Log skill gap analysis prompt
  - [ ] Log LLM response (top 3 skills)
  - [ ] Log LLM latency and token usage
  - [ ] Log code generation prompt and response
- [ ] **Task 4 (AC: 4):** Log aci.dev tool calls.
  - [ ] Log tool call requests (skill names)
  - [ ] Log tool call responses (course recommendations)
  - [ ] Log tool call latency
- [ ] **Task 5 (AC: 5):** Log code generation and validation.
  - [ ] Log generated code snippet
  - [ ] Log Daytona validation request
  - [ ] Log validation result (status, output, errors)
  - [ ] Log validation latency
- [ ] **Task 6 (AC: 6, 7):** Implement trace grouping and finalization.
  - [ ] Group all logs into single trace per request
  - [ ] Add trace metadata (user input, timestamp, total duration)
  - [ ] Finalize trace at end of request
  - [ ] Handle errors in logging (don't break main flow)

## Dev Notes

### Tech Stack

- **Backend Language:** Python 3.11+ [Source: architecture.md#tech-stack]
- **Backend Framework:** FastAPI [Source: architecture.md#tech-stack]
- **Observability:** Comet (Opik SDK) [Source: prd.md#technical-specifications]

### Comet (Opik) Requirements

From PRD S7:

> "As a developer, I want all RAG queries, LLM calls, and tool actions to be logged and tracked so I can debug and measure the agent's performance. Comet (Observability): Log the full trace, including latency, RAG context, and Daytona execution results."

**Key Requirements:**

- Log **all steps** of the workflow in a single trace
- Track **latency** for each component (Weaviate, FriendliAI, aci.dev, Daytona)
- Capture **context** (prompts, responses, documents)
- Enable **debugging** through detailed logs
- Provide **performance metrics** for optimization

**User Value (Developer Experience):**

- Debug issues quickly with complete trace visibility
- Measure performance of each component
- Identify bottlenecks in the workflow
- Track success/failure rates
- Optimize based on real usage data

### Opik SDK Integration

**Installation:**

```bash
pip install opik
```

**Basic Usage:**

```python
from opik import Opik
from opik.decorator import track

# Initialize Opik client
opik_client = Opik(
    api_key=os.getenv("COMET_API_KEY"),
    project_name=os.getenv("COMET_PROJECT_NAME", "careerpathai")
)

# Track a function
@track
def my_function():
    # Function code here
    pass

# Or manually create spans
with opik_client.trace(name="career_path_generation") as trace:
    with trace.span(name="weaviate_query") as span:
        # Query Weaviate
        span.log_input({"target_role": "ML Engineer"})
        span.log_output({"documents": 5})
```

### Trace Structure

```
Trace: career_path_generation
├── Span: weaviate_query
│   ├── Input: {target_role, limit, certainty}
│   ├── Output: {documents, count}
│   └── Duration: 0.5s
├── Span: friendli_skill_analysis
│   ├── Input: {current_role, target_role, knowledge_docs}
│   ├── Output: {skills: [skill1, skill2, skill3]}
│   └── Duration: 2.3s
├── Span: aci_course_search
│   ├── Input: {skills: [skill1, skill2, skill3]}
│   ├── Output: {courses_by_skill}
│   └── Duration: 1.2s
├── Span: code_generation
│   ├── Input: {skill: skill1}
│   ├── Output: {code, language, description}
│   └── Duration: 1.5s
├── Span: daytona_validation
│   ├── Input: {code, language, skill}
│   ├── Output: {status, output, execution_time}
│   └── Duration: 0.9s
└── Total Duration: 6.4s
```

### Data Flow

This is the final step in the core workflow:

1. Backend receives user input (Current Role, Target Role)
2. Backend queries Weaviate with Target Role (Story 1.2 - COMPLETED)
3. Weaviate returns relevant skill documents (Story 1.2 - COMPLETED)
4. Backend passes documents to FriendliAI for analysis (Story 1.3 - COMPLETED)
5. FriendliAI analyzes and returns top 3 skills (Story 1.3 - COMPLETED)
6. FriendliAI calls aci.dev tool for each skill (Story 1.4 - COMPLETED)
7. aci.dev executes `search_learning_content` and returns courses (Story 1.4 - COMPLETED)
8. Backend integrates course recommendations into response (Story 1.4 - COMPLETED)
9. Backend calls FriendliAI to generate code snippet for first skill (Story 1.5 - COMPLETED)
10. Backend integrates code snippet into response (Story 1.5 - COMPLETED)
11. Backend sends code to Daytona for validation (Story 1.6 - COMPLETED)
12. Daytona executes code and returns validation result (Story 1.6 - COMPLETED)
13. Backend integrates validation result into response (Story 1.6 - COMPLETED)
14. **→ Backend logs complete trace to Comet (THIS STORY)**
    [Source: architecture.md#core-workflows]

### Previous Story Insights

From all previous stories:

- **Story 1.2:** Weaviate queries return 5 documents with titles, descriptions, categories
- **Story 1.3:** FriendliAI returns 3 skills with prompts and responses
- **Story 1.4:** aci.dev returns course recommendations for each skill
- **Story 1.5:** Code generation returns code, language, description
- **Story 1.6:** Daytona validation returns status, output, errors, execution time

All of these should be logged to Comet for complete observability.

### Integration Points

- **Input:** Receives data from all previous workflow steps
- **Output:** Creates a complete trace in Comet dashboard
- **User Experience:** Developers can view traces in Comet UI for debugging
- **Performance:** Logging should not significantly impact response time (<100ms overhead)

### File Locations

- **Comet Client:** Create new file `api/comet_client.py` [Source: architecture.md#unified-project-structure]
- **Main Orchestrator:** Update `api/index.py` to add logging throughout workflow
- **Trace Context:** Use context manager or decorator pattern for clean integration

### Comet Client Implementation

Create `api/comet_client.py`:

```python
import os
import logging
from typing import Dict, Any, Optional, List
from contextlib import contextmanager

# Try to import Opik SDK
try:
    from opik import Opik
    from opik.decorator import track
    OPIK_SDK_AVAILABLE = True
except ImportError:
    OPIK_SDK_AVAILABLE = False

COMET_API_KEY = os.getenv("COMET_API_KEY")
COMET_PROJECT_NAME = os.getenv("COMET_PROJECT_NAME", "careerpathai")

class CometClient:
    """Client for Comet (Opik) observability."""

    def __init__(self):
        """Initialize Comet client."""
        self.api_key = COMET_API_KEY
        self.project_name = COMET_PROJECT_NAME
        self.client = None

        if OPIK_SDK_AVAILABLE and self.api_key:
            self.client = Opik(
                api_key=self.api_key,
                project_name=self.project_name
            )

    @contextmanager
    def trace_request(self, name: str, metadata: Dict[str, Any]):
        """Context manager for tracing a request."""
        # Implementation
        pass
```

### Logging Strategy

**What to Log:**

1. **Request Metadata**

   - User input (current role, target role)
   - Timestamp
   - Request ID

2. **Weaviate Query**

   - Query parameters
   - Retrieved documents (count, titles)
   - Latency

3. **FriendliAI Analysis**

   - Prompt text
   - Response (skills)
   - Token usage
   - Latency

4. **aci.dev Tool Calls**

   - Tool name and parameters
   - Response (courses)
   - Latency

5. **Code Generation**

   - Skill name
   - Generated code
   - Language
   - Latency

6. **Daytona Validation**

   - Code snippet
   - Validation result
   - Execution time
   - Latency

7. **Final Response**
   - Complete career path
   - Total duration
   - Success/failure status

**What NOT to Log:**

- API keys or secrets
- Personally identifiable information (PII)
- Large binary data
- Sensitive user data

### Error Handling

**Logging Failures Should Not Break Main Flow:**

```python
try:
    # Log to Comet
    comet_client.log_event(...)
except Exception as e:
    logger.warning(f"Failed to log to Comet: {str(e)}")
    # Continue with main workflow
```

**Fallback Behavior:**

- If Comet is unavailable, log to local logger only
- If API key is missing, skip Comet logging
- Never raise exceptions from logging code

### Performance Considerations

- **Logging Overhead:** Should be <100ms per request
- **Async Logging:** Use async operations where possible
- **Batch Logging:** Group logs and send in batches if needed
- **Sampling:** For high-volume production, consider sampling (not needed for MVP)

### Testing Requirements

- **Testing Strategy:** Manual testing for MVP, no automated tests required [Source: architecture.md#testing-strategy]

#### Test Categories

**1. Functional Tests (Core Functionality)**

- [ ] **TC-F1:** Log complete trace for successful request
  - Test: Complete workflow from input to response
  - Expected: All steps logged in Comet
  - Verify: Trace visible in Comet dashboard
- [ ] **TC-F2:** Log Weaviate retrieval
  - Expected: Query parameters and results logged
  - Verify: Document count and titles captured
- [ ] **TC-F3:** Log FriendliAI analysis
  - Expected: Prompt and response logged
  - Verify: Skills and latency captured
- [ ] **TC-F4:** Log aci.dev tool calls
  - Expected: Tool calls and responses logged
  - Verify: Course recommendations captured
- [ ] **TC-F5:** Log code generation and validation
  - Expected: Code and validation results logged
  - Verify: Execution details captured

**2. Integration Tests (System Integration)**

- [ ] **TC-I1:** End-to-end trace creation
  - Test: Complete workflow creates single trace
  - Expected: All spans grouped under one trace
  - Verify: Trace ID consistent across all logs
- [ ] **TC-I2:** Trace metadata
  - Expected: User input, timestamp, duration logged
  - Verify: Metadata visible in Comet dashboard
- [ ] **TC-I3:** Latency tracking
  - Expected: Each component's latency logged
  - Verify: Total duration = sum of component latencies

**3. Error Handling Tests (Resilience)**

- [ ] **TC-E1:** Handle Comet API unavailability
  - Simulate: Comet API down
  - Expected: Main workflow continues, logs to local logger
  - Verify: No exceptions raised
- [ ] **TC-E2:** Handle missing API key
  - Simulate: No COMET_API_KEY in environment
  - Expected: Logging skipped, workflow continues
  - Verify: Warning logged locally
- [ ] **TC-E3:** Handle logging failures
  - Simulate: Invalid log data
  - Expected: Error caught, workflow continues
  - Verify: Failure logged locally

**4. Performance Tests (Speed & Efficiency)**

- [ ] **TC-P1:** Logging overhead is minimal
  - Test: Measure request time with/without logging
  - Expected: Overhead <100ms
  - Verify: No significant performance impact
- [ ] **TC-P2:** Async logging doesn't block
  - Test: Logging doesn't delay response
  - Expected: Response sent before logging completes
  - Verify: User experience not affected

**5. Security Tests (Data Safety)**

- [ ] **TC-S1:** API keys not logged
  - Test: Check all logged data
  - Expected: No API keys in logs
  - Verify: Sensitive data filtered
- [ ] **TC-S2:** PII not logged
  - Test: Check for user data
  - Expected: No PII in logs
  - Verify: Data privacy maintained

**6. User Experience Tests (Developer UX)**

- [ ] **TC-U1:** Traces are easy to find
  - Test: Search for trace in Comet dashboard
  - Expected: Traces searchable by request ID or timestamp
  - Verify: Dashboard is user-friendly
- [ ] **TC-U2:** Logs are informative
  - Verify: Each log has clear description
  - Verify: Timing information is accurate
  - Test: Developer can debug issues from logs
- [ ] **TC-U3:** Error traces are highlighted
  - Test: Failed requests
  - Expected: Errors clearly marked in dashboard
  - Verify: Easy to identify failed requests

#### Acceptance Testing Checklist

Before marking story as "Done", verify:

- [ ] All functional tests pass (5/5)
- [ ] All integration tests pass (3/3)
- [ ] All error handling tests pass (3/3)
- [ ] Performance tests pass (2/2)
- [ ] All security tests pass (2/2)
- [ ] UX tests pass (minimum 2/3)
- [ ] Comet dashboard shows complete traces
- [ ] Product Owner approves logging implementation

### Environment Variables

Add to `.env`:

```bash
# Comet Configuration
COMET_API_KEY=your_comet_api_key_here
COMET_PROJECT_NAME=careerpathai
COMET_WORKSPACE=your_workspace_name
```

Add to `.env.example`:

```bash
# Comet Configuration
COMET_API_KEY=your_comet_api_key_here
COMET_PROJECT_NAME=careerpathai
COMET_WORKSPACE=your_workspace_name
```

### Dependencies

Add to `api/requirements.txt`:

```
opik>=0.1.0  # Comet's Opik SDK for LLM observability
```

### Example Trace Output

**Comet Dashboard View:**

```
Trace ID: trace_abc123
Duration: 6.4s
Status: Success

Spans:
├─ weaviate_query (0.5s)
│  Input: {target_role: "ML Engineer", limit: 5}
│  Output: {documents: 5, titles: [...]}
│
├─ friendli_skill_analysis (2.3s)
│  Input: {current_role: "Frontend Dev", target_role: "ML Engineer"}
│  Output: {skills: ["Machine Learning", "Python", "TensorFlow"]}
│  Tokens: {prompt: 450, completion: 120}
│
├─ aci_course_search (1.2s)
│  Input: {skills: ["Machine Learning", "Python", "TensorFlow"]}
│  Output: {courses: 9 total}
│
├─ code_generation (1.5s)
│  Input: {skill: "Machine Learning"}
│  Output: {code: "...", language: "python"}
│
└─ daytona_validation (0.9s)
   Input: {code: "...", language: "python"}
   Output: {status: "Success", execution_time: 0.234}
```

### Integration with Existing Code

**Minimal Changes Required:**

The logging should be added as a wrapper around existing code, not modifying the core logic:

```python
@app.post("/api/generate-career-path")
async def generate_career_path(user_input: UserInput):
    # Initialize Comet trace
    comet_client = get_comet_client()

    with comet_client.trace_request("career_path_generation", {
        "current_role": user_input.currentRole,
        "target_role": user_input.targetRole
    }):
        # Existing workflow code
        # Add logging calls at each step

        # Step 1: Weaviate
        comet_client.log_span("weaviate_query", input_data, output_data)

        # Step 2: FriendliAI
        comet_client.log_span("friendli_analysis", input_data, output_data)

        # ... etc
```

### Comet Dashboard Configuration

**Project Setup:**

1. Create project in Comet: "careerpathai"
2. Generate API key
3. Configure workspace
4. Set up custom dashboards for:
   - Request success rate
   - Average latency per component
   - Error rate tracking
   - Token usage monitoring

**Metrics to Track:**

- Total requests
- Success rate
- Average latency (total and per component)
- Error rate
- Token usage (FriendliAI)
- Code validation success rate

## Definition of Done

### Implementation Checklist

- [ ] `init_comet_client()` function implemented in `api/comet_client.py`
- [ ] Opik SDK installed and configured
- [ ] Trace initialization implemented
- [ ] Weaviate query logging implemented
- [ ] FriendliAI analysis logging implemented
- [ ] aci.dev tool call logging implemented
- [ ] Code generation logging implemented
- [ ] Daytona validation logging implemented
- [ ] Trace finalization implemented
- [ ] Integration with main orchestrator in `api/index.py`
- [ ] Environment variables configured

### Testing Checklist

- [ ] All functional tests pass (5/5)
- [ ] All integration tests pass (3/3)
- [ ] All error handling tests pass (3/3)
- [ ] All performance tests pass (2/2)
- [ ] All security tests pass (2/2)
- [ ] UX tests pass (minimum 2/3)
- [ ] Test script created for manual testing
- [ ] Comet dashboard verified

### Quality Checklist

- [ ] Logging is comprehensive and informative
- [ ] Traces are easy to navigate in dashboard
- [ ] Performance overhead is minimal (<100ms)
- [ ] Error handling doesn't break main flow
- [ ] Security is maintained (no sensitive data logged)

### Integration Checklist

- [ ] All workflow steps are logged
- [ ] Traces are grouped correctly
- [ ] Latency tracking is accurate
- [ ] Error handling provides graceful degradation
- [ ] Logging implemented for debugging
- [ ] Ready for production deployment

### Security Checklist

- [ ] API keys not logged
- [ ] PII not logged
- [ ] Sensitive data filtered
- [ ] All security tests pass

### Documentation Checklist

- [ ] Function docstrings complete
- [ ] Logging strategy documented
- [ ] Dashboard setup documented
- [ ] Testing procedures documented
- [ ] Known limitations documented

### Acceptance Checklist

- [ ] Product Owner review and approval
- [ ] Manual testing confirms logging works
- [ ] Comet dashboard shows complete traces
- [ ] Demo prepared showing end-to-end observability
- [ ] MVP is complete and ready for hackathon demo

## Change Log

| Date       | Version | Description                              | Author               |
| :--------- | :------ | :--------------------------------------- | :------------------- |
| 2025-10-02 | 1.0     | Initial draft                            | Scrum Master         |
| 2025-10-02 | 2.0     | Implementation completed - MVP Complete! | Full Stack Developer |
